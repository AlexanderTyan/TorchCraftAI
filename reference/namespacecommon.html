<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.11"/>
<title>TorchCraftAI: common Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">TorchCraftAI
   </div>
   <div id="projectbrief">A bot for machine learning research on StarCraft: Brood War</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.11 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="namespaces.html"><span>Namespace&#160;List</span></a></li>
      <li><a href="namespacemembers.html"><span>Namespace&#160;Members</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#namespaces">Namespaces</a> &#124;
<a href="#nested-classes">Classes</a> &#124;
<a href="#typedef-members">Typedefs</a> &#124;
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a> &#124;
<a href="#var-members">Variables</a>  </div>
  <div class="headertitle">
<div class="title">common Namespace Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>General utilities.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="namespaces"></a>
Namespaces</h2></td></tr>
<tr class="memitem:namespacecommon_1_1fsutils"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon_1_1fsutils.html">fsutils</a></td></tr>
<tr class="memdesc:namespacecommon_1_1fsutils"><td class="mdescLeft">&#160;</td><td class="mdescRight">Utility functions for interacting with the file system. <br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:namespacecommon_1_1zstd"><td class="memItemLeft" align="right" valign="top"> &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon_1_1zstd.html">zstd</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1AssertionFailure.html">AssertionFailure</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1BufferedConsumer.html">BufferedConsumer</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A simple producer/consumer class.  <a href="classcommon_1_1BufferedConsumer.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1BufferedProducer.html">BufferedProducer</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A simple producer class.  <a href="classcommon_1_1BufferedProducer.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1CircularBuffer.html">CircularBuffer</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1DataReader.html">DataReader</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A multi-threaded reader for cerealized data.  <a href="classcommon_1_1DataReader.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structcommon_1_1DataReader__NoTransform.html">DataReader_NoTransform</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1DataReaderIterator.html">DataReaderIterator</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A multi-threaded iterator that performs decerealization of objects and returns data in batches.  <a href="classcommon_1_1DataReaderIterator.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1DataReaderTransform.html">DataReaderTransform</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Wrapper for <a class="el" href="classcommon_1_1DataReaderIterator.html" title="A multi-threaded iterator that performs decerealization of objects and returns data in batches...">DataReaderIterator</a> that applies an additional transform to the resulting batches.  <a href="classcommon_1_1DataReaderTransform.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1Exception.html">Exception</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1IMembuf.html">IMembuf</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A stream buffer for reading from a vector of bytes.  <a href="classcommon_1_1IMembuf.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1LRUCache.html">LRUCache</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1OMembuf.html">OMembuf</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A stream buffer for writing to an accessible vector of bytes.  <a href="classcommon_1_1OMembuf.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1Rand.html">Rand</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classcommon_1_1ScopeGuard.html">ScopeGuard</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structcommon_1_1WeightSummary.html">WeightSummary</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">Collects metrics about a container's weights.  <a href="structcommon_1_1WeightSummary.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="typedef-members"></a>
Typedefs</h2></td></tr>
<tr class="memitem:adedd8ac6c51fc369a724bc5610779ffb"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#adedd8ac6c51fc369a724bc5610779ffb">VarList</a> = torch::autograd::variable_list</td></tr>
<tr class="separator:adedd8ac6c51fc369a724bc5610779ffb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c23509b882f8742d58c9c13e002702e"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a3c23509b882f8742d58c9c13e002702e">HookFunction</a> = std::function&lt; <a class="el" href="namespacecommon.html#adedd8ac6c51fc369a724bc5610779ffb">VarList</a>(const <a class="el" href="namespacecommon.html#adedd8ac6c51fc369a724bc5610779ffb">VarList</a> &amp;, const <a class="el" href="namespacecommon.html#adedd8ac6c51fc369a724bc5610779ffb">VarList</a> &amp;)&gt;</td></tr>
<tr class="separator:a3c23509b882f8742d58c9c13e002702e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a54cd622e23826275f4fc3b580c73dcf0"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a54cd622e23826275f4fc3b580c73dcf0">TensorTransform</a> = std::function&lt; torch::Tensor(torch::Tensor)&gt;</td></tr>
<tr class="memdesc:a54cd622e23826275f4fc3b580c73dcf0"><td class="mdescLeft">&#160;</td><td class="mdescRight">This is a convenience function to apply a tensor transformation to a complex type.  <a href="#a54cd622e23826275f4fc3b580c73dcf0">More...</a><br /></td></tr>
<tr class="separator:a54cd622e23826275f4fc3b580c73dcf0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8798ccb7b38e02568bc3766dfc43fe2f"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a8798ccb7b38e02568bc3766dfc43fe2f">DataReaderThreadInitF</a> = std::function&lt; void()&gt;</td></tr>
<tr class="separator:a8798ccb7b38e02568bc3766dfc43fe2f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a647d70706422d1ce42ca7be76bf0ca37"><td class="memItemLeft" align="right" valign="top">using&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a647d70706422d1ce42ca7be76bf0ca37">hires_clock</a> = std::chrono::steady_clock</td></tr>
<tr class="separator:a647d70706422d1ce42ca7be76bf0ca37"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:a5d2f6ab6c2580a0834320dc08c87e279"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a5d2f6ab6c2580a0834320dc08c87e279">PadType</a> { <a class="el" href="namespacecommon.html#a5d2f6ab6c2580a0834320dc08c87e279ad7ed4ee1df437474d005188535f74875">PadType::Zero</a>, 
<a class="el" href="namespacecommon.html#a5d2f6ab6c2580a0834320dc08c87e279aaea1e492943ccbad7ee270ec1e064758">PadType::Reflection</a>, 
<a class="el" href="namespacecommon.html#a5d2f6ab6c2580a0834320dc08c87e279a8c340dc334134096f68b880b42a8692c">PadType::Replication</a>
 }</td></tr>
<tr class="separator:a5d2f6ab6c2580a0834320dc08c87e279"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a95e9c4d7ebaf91bf709c582c45fa4ed6"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a95e9c4d7ebaf91bf709c582c45fa4ed6">ConcatType</a> { <a class="el" href="namespacecommon.html#a95e9c4d7ebaf91bf709c582c45fa4ed6a6adf97f83acf6453d4a6a4b1070f3754">ConcatType::None</a>, 
<a class="el" href="namespacecommon.html#a95e9c4d7ebaf91bf709c582c45fa4ed6a324118a6721dd6b8a9b9f4e327df2bf5">ConcatType::Input</a>, 
<a class="el" href="namespacecommon.html#a95e9c4d7ebaf91bf709c582c45fa4ed6a2403def5083f02105e7802b3b315681e">ConcatType::Mirror</a>
 }</td></tr>
<tr class="separator:a95e9c4d7ebaf91bf709c582c45fa4ed6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae0762fe221f7fddd392f24cbae4d649c"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ae0762fe221f7fddd392f24cbae4d649c">UpsamplingType</a> { <a class="el" href="namespacecommon.html#ae0762fe221f7fddd392f24cbae4d649ca6adf97f83acf6453d4a6a4b1070f3754">UpsamplingType::None</a>, 
<a class="el" href="namespacecommon.html#ae0762fe221f7fddd392f24cbae4d649ca18c441822997cef7f104da468455fb88">UpsamplingType::Bilin</a>, 
<a class="el" href="namespacecommon.html#ae0762fe221f7fddd392f24cbae4d649cad0f139c3cd10333e40bd51299c64d30a">UpsamplingType::Deconv</a>
 }</td></tr>
<tr class="separator:ae0762fe221f7fddd392f24cbae4d649c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5ce7b02761744f38a15673e3fe0a4552"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a5ce7b02761744f38a15673e3fe0a4552">DecodeType</a> { <a class="el" href="namespacecommon.html#a5ce7b02761744f38a15673e3fe0a4552a6adf97f83acf6453d4a6a4b1070f3754">DecodeType::None</a>, 
<a class="el" href="namespacecommon.html#a5ce7b02761744f38a15673e3fe0a4552acc31a669f6d086f816852dda3290a3ef">DecodeType::Conv</a>, 
<a class="el" href="namespacecommon.html#a5ce7b02761744f38a15673e3fe0a4552ad0f139c3cd10333e40bd51299c64d30a">DecodeType::Deconv</a>
 }</td></tr>
<tr class="separator:a5ce7b02761744f38a15673e3fe0a4552"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6c60ad4086ad27183b4794f0844e73d1"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a6c60ad4086ad27183b4794f0844e73d1">DilationScheme</a> { <a class="el" href="namespacecommon.html#a6c60ad4086ad27183b4794f0844e73d1a6adf97f83acf6453d4a6a4b1070f3754">DilationScheme::None</a>, 
<a class="el" href="namespacecommon.html#a6c60ad4086ad27183b4794f0844e73d1a32a843da6ea40ab3b17a3421ccdf671b">DilationScheme::Linear</a>, 
<a class="el" href="namespacecommon.html#a6c60ad4086ad27183b4794f0844e73d1ac1e19c09f700938f0ff7f1fd4722a3ac">DilationScheme::Exponential</a>
 }</td></tr>
<tr class="separator:a6c60ad4086ad27183b4794f0844e73d1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aee0ad6b31a5c2edcebea2c9745660725"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725">UpsampleMode</a> { <a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725a60494f02d440f316319dd0fad40ad007">UpsampleMode::Nearest</a>, 
<a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725a32a843da6ea40ab3b17a3421ccdf671b">UpsampleMode::Linear</a>, 
<a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725aaf17c98bbd83c27d6426d2ff3fa81d7f">UpsampleMode::Bilinear</a>, 
<a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725a969a7b6f1f3951ef293b3ff281dc293d">UpsampleMode::Trilinear</a>
 }<tr class="memdesc:aee0ad6b31a5c2edcebea2c9745660725"><td class="mdescLeft">&#160;</td><td class="mdescRight">Mimics pytorch's upsample function.  <a href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725">More...</a><br /></td></tr>
</td></tr>
<tr class="separator:aee0ad6b31a5c2edcebea2c9745660725"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a328822315f7b76ea5fd7afe902a9d174"><td class="memItemLeft" align="right" valign="top">backward::StackTrace&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a328822315f7b76ea5fd7afe902a9d174">createStackTrace</a> ()</td></tr>
<tr class="separator:a328822315f7b76ea5fd7afe902a9d174"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6403cdbd861177e3001b494300609130"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a6403cdbd861177e3001b494300609130">tensorInfo</a> (torch::Tensor x)</td></tr>
<tr class="memdesc:a6403cdbd861177e3001b494300609130"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a string containing the tensor type and sizes.  <a href="#a6403cdbd861177e3001b494300609130">More...</a><br /></td></tr>
<tr class="separator:a6403cdbd861177e3001b494300609130"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a182601d099eaa2a84e1bca8cc8257bd8"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a182601d099eaa2a84e1bca8cc8257bd8">variantInfo</a> (ag::Variant x)</td></tr>
<tr class="memdesc:a182601d099eaa2a84e1bca8cc8257bd8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a string describing the content of a variant.  <a href="#a182601d099eaa2a84e1bca8cc8257bd8">More...</a><br /></td></tr>
<tr class="separator:a182601d099eaa2a84e1bca8cc8257bd8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5c00b2018ce9c66e0aea9248c7864378"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a5c00b2018ce9c66e0aea9248c7864378">tensorStats</a> (torch::Tensor x)</td></tr>
<tr class="memdesc:a5c00b2018ce9c66e0aea9248c7864378"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a string containing the tensor info, the max/min/mean and sum.  <a href="#a5c00b2018ce9c66e0aea9248c7864378">More...</a><br /></td></tr>
<tr class="separator:a5c00b2018ce9c66e0aea9248c7864378"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aff3cdc4a58d5aaab088d23646d57da47"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#aff3cdc4a58d5aaab088d23646d57da47">checkTensor</a> (torch::Tensor x, bool logOnError=true)</td></tr>
<tr class="memdesc:aff3cdc4a58d5aaab088d23646d57da47"><td class="mdescLeft">&#160;</td><td class="mdescRight">Throws if the given float tensor has a NaN or +/- infinity.  <a href="#aff3cdc4a58d5aaab088d23646d57da47">More...</a><br /></td></tr>
<tr class="separator:aff3cdc4a58d5aaab088d23646d57da47"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afa92f86a81bea052e6adad0c0c39f281"><td class="memItemLeft" align="right" valign="top">torch::Tensor const &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#afa92f86a81bea052e6adad0c0c39f281">addHook</a> (torch::Tensor const &amp;tensor, <a class="el" href="namespacecommon.html#a3c23509b882f8742d58c9c13e002702e">HookFunction</a> &amp;&amp;f)</td></tr>
<tr class="memdesc:afa92f86a81bea052e6adad0c0c39f281"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a hook to the backwards of the variable.  <a href="#afa92f86a81bea052e6adad0c0c39f281">More...</a><br /></td></tr>
<tr class="separator:afa92f86a81bea052e6adad0c0c39f281"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1722115063aa872f70355ab48c2b0c4d"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a1722115063aa872f70355ab48c2b0c4d">assertSize</a> (const std::string &amp;name, const torch::Tensor &amp;tensor, at::IntList sizes)</td></tr>
<tr class="memdesc:a1722115063aa872f70355ab48c2b0c4d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Verifies that a tensor's dimension sizes match expectations.  <a href="#a1722115063aa872f70355ab48c2b0c4d">More...</a><br /></td></tr>
<tr class="separator:a1722115063aa872f70355ab48c2b0c4d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab3207d3b5ed49cd34d43d779a1c5bfda"><td class="memItemLeft" align="right" valign="top">std::ostream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ab3207d3b5ed49cd34d43d779a1c5bfda">operator&lt;&lt;</a> (std::ostream &amp;out, const <a class="el" href="structcommon_1_1WeightSummary.html">WeightSummary</a> &amp;summary)</td></tr>
<tr class="separator:ab3207d3b5ed49cd34d43d779a1c5bfda"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aafe019d691deac5c07646f046796bc9f"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#aafe019d691deac5c07646f046796bc9f">normalPDF</a> (torch::Tensor x, torch::Tensor mean, torch::Tensor std)</td></tr>
<tr class="memdesc:aafe019d691deac5c07646f046796bc9f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the PDF of the normal law.  <a href="#aafe019d691deac5c07646f046796bc9f">More...</a><br /></td></tr>
<tr class="separator:aafe019d691deac5c07646f046796bc9f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af47e9d6eb984101dd1ca8602260acf7b"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#af47e9d6eb984101dd1ca8602260acf7b">normalPDF</a> (torch::Tensor x, torch::Tensor mean, double std)</td></tr>
<tr class="separator:af47e9d6eb984101dd1ca8602260acf7b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3d897b66ebb1c87646c385f31de4c99c"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a3d897b66ebb1c87646c385f31de4c99c">AUTOGRAD_CONTAINER_CLASS</a> (MLP)</td></tr>
<tr class="memdesc:a3d897b66ebb1c87646c385f31de4c99c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simple MLP of nLayers layers, with hidden size all being the same.  <a href="#a3d897b66ebb1c87646c385f31de4c99c">More...</a><br /></td></tr>
<tr class="separator:a3d897b66ebb1c87646c385f31de4c99c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a18d21af0bb1a1bd51acf6589eaf8fc05"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a18d21af0bb1a1bd51acf6589eaf8fc05">AUTOGRAD_CONTAINER_CLASS</a> (GatedConv)</td></tr>
<tr class="separator:a18d21af0bb1a1bd51acf6589eaf8fc05"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6d25c1fff03a1b36a30cd19be24b2241"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a6d25c1fff03a1b36a30cd19be24b2241">AUTOGRAD_CONTAINER_CLASS</a> (ConvBlock)</td></tr>
<tr class="memdesc:a6d25c1fff03a1b36a30cd19be24b2241"><td class="mdescLeft">&#160;</td><td class="mdescRight">Simple convolutional block, with optional residual connection From a user perspective, the convolution parameters behave the same as if the block was a single conv layer.  <a href="#a6d25c1fff03a1b36a30cd19be24b2241">More...</a><br /></td></tr>
<tr class="separator:a6d25c1fff03a1b36a30cd19be24b2241"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aedab797eb4cdaf2aaf86b69f3ec3f44b"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#aedab797eb4cdaf2aaf86b69f3ec3f44b">AUTOGRAD_CONTAINER_CLASS</a> (EncoderDecoder)</td></tr>
<tr class="separator:aedab797eb4cdaf2aaf86b69f3ec3f44b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1f46d58d0ede06bfef969550066be329"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a1f46d58d0ede06bfef969550066be329">AUTOGRAD_CONTAINER_CLASS</a> (MHAttention)</td></tr>
<tr class="separator:a1f46d58d0ede06bfef969550066be329"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa6c41f5406c455f82089b64539ccde55"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#aa6c41f5406c455f82089b64539ccde55">repeat2d</a> (torch::Tensor data, at::IntList sizes)</td></tr>
<tr class="memdesc:aa6c41f5406c455f82089b64539ccde55"><td class="mdescLeft">&#160;</td><td class="mdescRight">Repeat a 1D tensor so that you end up with a (#channels, sizes[0], size[1]) tensor.  <a href="#aa6c41f5406c455f82089b64539ccde55">More...</a><br /></td></tr>
<tr class="separator:aa6c41f5406c455f82089b64539ccde55"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1d8c2672f626cd80d6dfb68c26fef6b7"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a1d8c2672f626cd80d6dfb68c26fef6b7">scatterSum2d</a> (torch::Tensor positions, torch::Tensor data, at::IntList sizes)</td></tr>
<tr class="memdesc:a1d8c2672f626cd80d6dfb68c26fef6b7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Scatter data into dest at given positions.  <a href="#a1d8c2672f626cd80d6dfb68c26fef6b7">More...</a><br /></td></tr>
<tr class="separator:a1d8c2672f626cd80d6dfb68c26fef6b7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a927751b4aecba3af139b560226d25d32"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a927751b4aecba3af139b560226d25d32">makeBatch</a> (ag::tensor_list const &amp;, double pad=0)</td></tr>
<tr class="memdesc:a927751b4aecba3af139b560226d25d32"><td class="mdescLeft">&#160;</td><td class="mdescRight">Equivalent to a stack along dim 0 of the input, but with the values padded correct so the size is rectangular.  <a href="#a927751b4aecba3af139b560226d25d32">More...</a><br /></td></tr>
<tr class="separator:a927751b4aecba3af139b560226d25d32"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0f02d76dc0416dbe2e684e40eda6fc92"><td class="memItemLeft" align="right" valign="top">ag::Variant&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a0f02d76dc0416dbe2e684e40eda6fc92">makeBatchVariant</a> (const std::vector&lt; ag::Variant &gt; &amp;queries, double pad=0)</td></tr>
<tr class="memdesc:a0f02d76dc0416dbe2e684e40eda6fc92"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function works similarly as makeBatch but handles more input types.  <a href="#a0f02d76dc0416dbe2e684e40eda6fc92">More...</a><br /></td></tr>
<tr class="separator:a0f02d76dc0416dbe2e684e40eda6fc92"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af6875c702673bffa084bc33a2b88d997"><td class="memItemLeft" align="right" valign="top">std::vector&lt; ag::Variant &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#af6875c702673bffa084bc33a2b88d997">unBatchVariant</a> (ag::Variant batch, int stride=1, bool maskOut=false, double maskValue=-1)</td></tr>
<tr class="memdesc:af6875c702673bffa084bc33a2b88d997"><td class="mdescLeft">&#160;</td><td class="mdescRight">This function is the opposite of makeBatchVariant.  <a href="#af6875c702673bffa084bc33a2b88d997">More...</a><br /></td></tr>
<tr class="separator:af6875c702673bffa084bc33a2b88d997"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0206c43d70412a65070cc5fe14c31104"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a0206c43d70412a65070cc5fe14c31104">pad2d</a> (torch::Tensor input, at::IntList pad)</td></tr>
<tr class="memdesc:a0206c43d70412a65070cc5fe14c31104"><td class="mdescLeft">&#160;</td><td class="mdescRight">Zero-padding (only supports 3d input)  <a href="#a0206c43d70412a65070cc5fe14c31104">More...</a><br /></td></tr>
<tr class="separator:a0206c43d70412a65070cc5fe14c31104"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8491908e5c44ef8d3b1dcb26de9a4c68"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a8491908e5c44ef8d3b1dcb26de9a4c68">padNd</a> (torch::Tensor input, at::IntList pad)</td></tr>
<tr class="memdesc:a8491908e5c44ef8d3b1dcb26de9a4c68"><td class="mdescLeft">&#160;</td><td class="mdescRight">Zero-padding (for any number of dimensions) For every dimensions of the input, pad contains 2 elements: the padding before and after along this dimension.  <a href="#a8491908e5c44ef8d3b1dcb26de9a4c68">More...</a><br /></td></tr>
<tr class="separator:a8491908e5c44ef8d3b1dcb26de9a4c68"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaa55ed98e60017f41cddd7936c7468e2"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#aaa55ed98e60017f41cddd7936c7468e2">flip</a> (torch::Tensor x, int dim)</td></tr>
<tr class="memdesc:aaa55ed98e60017f41cddd7936c7468e2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Flips a tensor along a given dimension.  <a href="#aaa55ed98e60017f41cddd7936c7468e2">More...</a><br /></td></tr>
<tr class="separator:aaa55ed98e60017f41cddd7936c7468e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8711a9cfdf20b614f9d2236903252b0"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#af8711a9cfdf20b614f9d2236903252b0">upsample</a> (torch::Tensor input, <a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725">UpsampleMode</a> mode, at::IntList size)</td></tr>
<tr class="separator:af8711a9cfdf20b614f9d2236903252b0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae7d6f79d3075016c8ebfd2bb43857d93"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ae7d6f79d3075016c8ebfd2bb43857d93">upsample</a> (torch::Tensor input, <a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725">UpsampleMode</a> mode, int scaleFactor)</td></tr>
<tr class="separator:ae7d6f79d3075016c8ebfd2bb43857d93"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4d08e8aa3221955ce42cc61e6424c26c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a4d08e8aa3221955ce42cc61e6424c26c">zerosToOnes_</a> (torch::Tensor x)</td></tr>
<tr class="memdesc:a4d08e8aa3221955ce42cc61e6424c26c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Replace (in-place) all zeroes of x by ones.  <a href="#a4d08e8aa3221955ce42cc61e6424c26c">More...</a><br /></td></tr>
<tr class="separator:a4d08e8aa3221955ce42cc61e6424c26c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abf55d6112d2ad4bcde92acb7098fdabe"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#abf55d6112d2ad4bcde92acb7098fdabe">tensorFromNpyArray</a> (cnpy::NpyArray array, torch::TensorOptions op)</td></tr>
<tr class="separator:abf55d6112d2ad4bcde92acb7098fdabe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0eb9d40d761b11d3f9d6dd99774af699"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a0eb9d40d761b11d3f9d6dd99774af699">squash</a> (torch::Tensor x, int i, int j)</td></tr>
<tr class="memdesc:a0eb9d40d761b11d3f9d6dd99774af699"><td class="mdescLeft">&#160;</td><td class="mdescRight">Squash contiguous dimensions of a tensor into a single dimension.  <a href="#a0eb9d40d761b11d3f9d6dd99774af699">More...</a><br /></td></tr>
<tr class="separator:a0eb9d40d761b11d3f9d6dd99774af699"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3f4a250cfa835be53f41c68434c8d623"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a3f4a250cfa835be53f41c68434c8d623">unsquash</a> (torch::Tensor x, int i, at::IntList sizes)</td></tr>
<tr class="memdesc:a3f4a250cfa835be53f41c68434c8d623"><td class="mdescLeft">&#160;</td><td class="mdescRight">Unsquash a dimension of a tensor into several dimensions.  <a href="#a3f4a250cfa835be53f41c68434c8d623">More...</a><br /></td></tr>
<tr class="separator:a3f4a250cfa835be53f41c68434c8d623"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c7fced80a0a71cdfd8a464ab71243a8"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a3c7fced80a0a71cdfd8a464ab71243a8">maskedSum</a> (torch::Tensor x, torch::Tensor mask)</td></tr>
<tr class="memdesc:a3c7fced80a0a71cdfd8a464ab71243a8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Sum x across non-masked indices.  <a href="#a3c7fced80a0a71cdfd8a464ab71243a8">More...</a><br /></td></tr>
<tr class="separator:a3c7fced80a0a71cdfd8a464ab71243a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a596edcc9ffdf5ddd17f2872c9dee4771"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a596edcc9ffdf5ddd17f2872c9dee4771">maskedMean</a> (torch::Tensor x, torch::Tensor mask)</td></tr>
<tr class="memdesc:a596edcc9ffdf5ddd17f2872c9dee4771"><td class="mdescLeft">&#160;</td><td class="mdescRight">Average x over non-masked indices, returning 0 if all indices are masked.  <a href="#a596edcc9ffdf5ddd17f2872c9dee4771">More...</a><br /></td></tr>
<tr class="separator:a596edcc9ffdf5ddd17f2872c9dee4771"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa3a5ffdd16f2fbc577c2e7585826437d"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#aa3a5ffdd16f2fbc577c2e7585826437d">mseLoss</a> (torch::Tensor x, torch::Tensor y, torch::Tensor mask, bool sizeAverage=true, bool reduce=true)</td></tr>
<tr class="memdesc:aa3a5ffdd16f2fbc577c2e7585826437d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Computes the MSE loss between x and y.  <a href="#aa3a5ffdd16f2fbc577c2e7585826437d">More...</a><br /></td></tr>
<tr class="separator:aa3a5ffdd16f2fbc577c2e7585826437d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a521443beb7c0ef5abf09b5b27bf53b98"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a521443beb7c0ef5abf09b5b27bf53b98">crossEntropyLoss</a> (torch::Tensor input, int dim, torch::Tensor target, torch::Tensor weight, torch::Tensor mask, Reduction::Reduction reduction)</td></tr>
<tr class="separator:a521443beb7c0ef5abf09b5b27bf53b98"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af0f332c7ece355863f008a6926bca6b1"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#af0f332c7ece355863f008a6926bca6b1">nllLoss</a> (torch::Tensor input, int dim, torch::Tensor target, torch::Tensor weight, torch::Tensor mask, Reduction::Reduction reduction)</td></tr>
<tr class="separator:af0f332c7ece355863f008a6926bca6b1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0cedd9ef57020257a649a35cda020ec6"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a0cedd9ef57020257a649a35cda020ec6">clipGradientNorms</a> (std::vector&lt; torch::Tensor &gt; parameters, float maxNorm)</td></tr>
<tr class="memdesc:a0cedd9ef57020257a649a35cda020ec6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Rescale gradients so that the norm of all gradients (concatenated) is smaller than maxNorm.  <a href="#a0cedd9ef57020257a649a35cda020ec6">More...</a><br /></td></tr>
<tr class="separator:a0cedd9ef57020257a649a35cda020ec6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af50d71f082fad6c77806afb11e7977ed"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#af50d71f082fad6c77806afb11e7977ed">maskedSoftmax</a> (torch::Tensor input, torch::Tensor mask, int dim, float clampEpsilon=0)</td></tr>
<tr class="memdesc:af50d71f082fad6c77806afb11e7977ed"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute a masked softmax of a tensor in a numerically stable way by removing the max value before exponentiating.  <a href="#af50d71f082fad6c77806afb11e7977ed">More...</a><br /></td></tr>
<tr class="separator:af50d71f082fad6c77806afb11e7977ed"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0f52ff6fc24f7464958070dd246611b4"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; torch::Tensor, torch::Tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a0f52ff6fc24f7464958070dd246611b4">maskedMax</a> (torch::Tensor input, torch::Tensor mask, int dim, bool keepDim=false)</td></tr>
<tr class="memdesc:a0f52ff6fc24f7464958070dd246611b4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute a masked max/argmax of a tensor.  <a href="#a0f52ff6fc24f7464958070dd246611b4">More...</a><br /></td></tr>
<tr class="separator:a0f52ff6fc24f7464958070dd246611b4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae4caa699050fa90de074d7e0a506754a"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ae4caa699050fa90de074d7e0a506754a">weightedMaskedSoftmax</a> (torch::Tensor input, torch::Tensor mask, int dim, float clampEpsilon=0)</td></tr>
<tr class="memdesc:ae4caa699050fa90de074d7e0a506754a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute a weighted masked softmax of a tensor in a numerically stable way by removing the max value before exponentiating.  <a href="#ae4caa699050fa90de074d7e0a506754a">More...</a><br /></td></tr>
<tr class="separator:ae4caa699050fa90de074d7e0a506754a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad93f6d196233fabf025480c20f3d6b2a"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ad93f6d196233fabf025480c20f3d6b2a">selectIndex</a> (torch::Tensor x, torch::Tensor y, int axis, bool keepDim)</td></tr>
<tr class="separator:ad93f6d196233fabf025480c20f3d6b2a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2028ee778450a8f8c61a82d3990acea7"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a2028ee778450a8f8c61a82d3990acea7">extendIndex</a> (torch::Tensor y, int axis, int d)</td></tr>
<tr class="memdesc:a2028ee778450a8f8c61a82d3990acea7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Returns a byte tensor x such that selectIndex(x, y, axis) are only 1s.  <a href="#a2028ee778450a8f8c61a82d3990acea7">More...</a><br /></td></tr>
<tr class="separator:a2028ee778450a8f8c61a82d3990acea7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab40691e6a2589e90b0217ab16bf71a1d"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ab40691e6a2589e90b0217ab16bf71a1d">maskedCopy_</a> (torch::Tensor x, torch::Tensor mask, torch::Tensor source)</td></tr>
<tr class="memdesc:ab40691e6a2589e90b0217ab16bf71a1d"><td class="mdescLeft">&#160;</td><td class="mdescRight">For 1D tensors, this is equivalent to: x[i] &lt;- source[i] if mask[i] == 1.  <a href="#ab40691e6a2589e90b0217ab16bf71a1d">More...</a><br /></td></tr>
<tr class="separator:ab40691e6a2589e90b0217ab16bf71a1d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acaa59964ea24462c5299cd13d50fac9e"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#acaa59964ea24462c5299cd13d50fac9e">maskedCopy</a> (torch::Tensor x, torch::Tensor mask, torch::Tensor source)</td></tr>
<tr class="memdesc:acaa59964ea24462c5299cd13d50fac9e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Immutable masked copy (equivalent to x.clone().<a class="el" href="namespacecommon.html#ab40691e6a2589e90b0217ab16bf71a1d" title="For 1D tensors, this is equivalent to: x[i] &lt;- source[i] if mask[i] == 1. ">maskedCopy_()</a>).  <a href="#acaa59964ea24462c5299cd13d50fac9e">More...</a><br /></td></tr>
<tr class="separator:acaa59964ea24462c5299cd13d50fac9e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3d29882328849ffa17e1223e28d3a154"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a3d29882328849ffa17e1223e28d3a154">putNd_</a> (torch::Tensor x, torch::Tensor index, torch::Tensor source, bool accumulate=false)</td></tr>
<tr class="memdesc:a3d29882328849ffa17e1223e28d3a154"><td class="mdescLeft">&#160;</td><td class="mdescRight">Copies elements from source into x at positions determined by index.  <a href="#a3d29882328849ffa17e1223e28d3a154">More...</a><br /></td></tr>
<tr class="separator:a3d29882328849ffa17e1223e28d3a154"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae28cd3961778cddfa278b9ec8a9a7d8f"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ae28cd3961778cddfa278b9ec8a9a7d8f">takeNd</a> (torch::Tensor x, torch::Tensor index)</td></tr>
<tr class="memdesc:ae28cd3961778cddfa278b9ec8a9a7d8f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Inverse operation of putNd_.  <a href="#ae28cd3961778cddfa278b9ec8a9a7d8f">More...</a><br /></td></tr>
<tr class="separator:ae28cd3961778cddfa278b9ec8a9a7d8f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a06a7d0810e5cfcf017cb55534743f2ad"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a06a7d0810e5cfcf017cb55534743f2ad">indexMean</a> (int size, int dim, torch::Tensor index, torch::Tensor source)</td></tr>
<tr class="memdesc:a06a7d0810e5cfcf017cb55534743f2ad"><td class="mdescLeft">&#160;</td><td class="mdescRight">Like zeros.index_add_ but with the mean.  <a href="#a06a7d0810e5cfcf017cb55534743f2ad">More...</a><br /></td></tr>
<tr class="separator:a06a7d0810e5cfcf017cb55534743f2ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1a22830e6cc488dfba4e61bdde64d127"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a1a22830e6cc488dfba4e61bdde64d127">unsqueezes</a> (int before, torch::Tensor x, int after)</td></tr>
<tr class="memdesc:a1a22830e6cc488dfba4e61bdde64d127"><td class="mdescLeft">&#160;</td><td class="mdescRight">Do multiple unsqueezes on first and last dimensions.  <a href="#a1a22830e6cc488dfba4e61bdde64d127">More...</a><br /></td></tr>
<tr class="separator:a1a22830e6cc488dfba4e61bdde64d127"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae17e8aba9ee9fcc37164ac86fce0b52b"><td class="memItemLeft" align="right" valign="top">torch::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ae17e8aba9ee9fcc37164ac86fce0b52b">meshGrid</a> (ag::tensor_list tensors)</td></tr>
<tr class="memdesc:ae17e8aba9ee9fcc37164ac86fce0b52b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Takes N 1D tensors xi of size Xi and returns a tensor y of size X1 x ...  <a href="#ae17e8aba9ee9fcc37164ac86fce0b52b">More...</a><br /></td></tr>
<tr class="separator:ae17e8aba9ee9fcc37164ac86fce0b52b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2ab3d9995a952d2e4c4c2bab436f0541"><td class="memItemLeft" align="right" valign="top">ag::Variant&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a2ab3d9995a952d2e4c4c2bab436f0541">applyTransform</a> (ag::Variant input, const <a class="el" href="namespacecommon.html#a54cd622e23826275f4fc3b580c73dcf0">TensorTransform</a> &amp;fun)</td></tr>
<tr class="separator:a2ab3d9995a952d2e4c4c2bab436f0541"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a048a13249fa22958d877f55a96f8c585"><td class="memItemLeft" align="right" valign="top">at::Device&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a048a13249fa22958d877f55a96f8c585">getVariantDevice</a> (ag::Variant x)</td></tr>
<tr class="memdesc:a048a13249fa22958d877f55a96f8c585"><td class="mdescLeft">&#160;</td><td class="mdescRight">Utility to get the device of a variant.  <a href="#a048a13249fa22958d877f55a96f8c585">More...</a><br /></td></tr>
<tr class="separator:a048a13249fa22958d877f55a96f8c585"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4bdcc11eaa0149740bea8a2503662758"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a4bdcc11eaa0149740bea8a2503662758">gpuAvailable</a> ()</td></tr>
<tr class="memdesc:a4bdcc11eaa0149740bea8a2503662758"><td class="mdescLeft">&#160;</td><td class="mdescRight">Checks if a CUDA GPU is available.  <a href="#a4bdcc11eaa0149740bea8a2503662758">More...</a><br /></td></tr>
<tr class="separator:a4bdcc11eaa0149740bea8a2503662758"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a63e26830a3ec74de71978e51f2163de3"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a63e26830a3ec74de71978e51f2163de3">toHex</a> (std::vector&lt; uint8_t &gt; const &amp;digest)</td></tr>
<tr class="separator:a63e26830a3ec74de71978e51f2163de3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a38c1a0bfca2038eb61540f16b4f3bb39"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint8_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a38c1a0bfca2038eb61540f16b4f3bb39">sha256sum</a> (void const *data, size_t len)</td></tr>
<tr class="separator:a38c1a0bfca2038eb61540f16b4f3bb39"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a943de486e4b8b0fa227d44caa64702cd"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint8_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a943de486e4b8b0fa227d44caa64702cd">md5sum</a> (void const *data, size_t len)</td></tr>
<tr class="separator:a943de486e4b8b0fa227d44caa64702cd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1de170b4b067d43d139ae986bf78c842"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint8_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a1de170b4b067d43d139ae986bf78c842">sha256sum</a> (std::string_view data)</td></tr>
<tr class="separator:a1de170b4b067d43d139ae986bf78c842"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5c52510f1a58f2330aa65d0800f78395"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint8_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a5c52510f1a58f2330aa65d0800f78395">sha256sum</a> (std::vector&lt; uint8_t &gt; const &amp;data)</td></tr>
<tr class="separator:a5c52510f1a58f2330aa65d0800f78395"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0715f0339bc9259e43162035e313c204"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint8_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a0715f0339bc9259e43162035e313c204">md5sum</a> (std::string_view data)</td></tr>
<tr class="separator:a0715f0339bc9259e43162035e313c204"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4963b461ac47972bfa158fb47d51a281"><td class="memItemLeft" align="right" valign="top">std::vector&lt; uint8_t &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a4963b461ac47972bfa158fb47d51a281">md5sum</a> (std::vector&lt; uint8_t &gt; const &amp;data)</td></tr>
<tr class="separator:a4963b461ac47972bfa158fb47d51a281"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac1a19e3dbacb1000f49d696cec022e30"><td class="memTemplParams" colspan="2">template&lt;typename T , typename F &gt; </td></tr>
<tr class="memitem:ac1a19e3dbacb1000f49d696cec022e30"><td class="memTemplItemLeft" align="right" valign="top">std::unique_ptr&lt; <a class="el" href="classcommon_1_1DataReaderTransform.html">DataReaderTransform</a>&lt; T, F &gt; &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ac1a19e3dbacb1000f49d696cec022e30">makeDataReaderTransform</a> (std::unique_ptr&lt; <a class="el" href="classcommon_1_1DataReaderIterator.html">DataReaderIterator</a>&lt; T &gt;&gt; &amp;&amp;it, F &amp;&amp;function, <a class="el" href="namespacecommon.html#a8798ccb7b38e02568bc3766dfc43fe2f">DataReaderThreadInitF</a> init=<a class="el" href="namespacecommon.html#a8909741e72f3f5df7ad5e922ac088930">DataReader_NoopF</a>)</td></tr>
<tr class="separator:ac1a19e3dbacb1000f49d696cec022e30"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0812ae93030aec1c6d9d4d873fe29271"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a0812ae93030aec1c6d9d4d873fe29271"><td class="memTemplItemLeft" align="right" valign="top">auto&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a0812ae93030aec1c6d9d4d873fe29271">makeDataReader</a> (std::vector&lt; std::string &gt; paths, size_t numThreads, size_t batchSize, std::string pathPrefix=std::string(), <a class="el" href="namespacecommon.html#a8798ccb7b38e02568bc3766dfc43fe2f">DataReaderThreadInitF</a> init=<a class="el" href="namespacecommon.html#a8909741e72f3f5df7ad5e922ac088930">DataReader_NoopF</a>)</td></tr>
<tr class="separator:a0812ae93030aec1c6d9d4d873fe29271"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac749eb34bb70eb7a2e383f499c781f48"><td class="memTemplParams" colspan="2">template&lt;typename T , typename F &gt; </td></tr>
<tr class="memitem:ac749eb34bb70eb7a2e383f499c781f48"><td class="memTemplItemLeft" align="right" valign="top">auto&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ac749eb34bb70eb7a2e383f499c781f48">makeDataReader</a> (std::vector&lt; std::string &gt; paths, size_t numThreads, size_t batchSize, F transform, std::string pathPrefix=std::string(), <a class="el" href="namespacecommon.html#a8798ccb7b38e02568bc3766dfc43fe2f">DataReaderThreadInitF</a> init=<a class="el" href="namespacecommon.html#a8909741e72f3f5df7ad5e922ac088930">DataReader_NoopF</a>)</td></tr>
<tr class="separator:ac749eb34bb70eb7a2e383f499c781f48"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a34d4ac17f6acdebb57a44e605866dda9"><td class="memTemplParams" colspan="2">template&lt;typename Enumeration &gt; </td></tr>
<tr class="memitem:a34d4ac17f6acdebb57a44e605866dda9"><td class="memTemplItemLeft" align="right" valign="top">auto&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a34d4ac17f6acdebb57a44e605866dda9">enumAsInt</a> (Enumeration const value) -&gt; typename std::underlying_type&lt; Enumeration &gt;::type</td></tr>
<tr class="separator:a34d4ac17f6acdebb57a44e605866dda9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3a3287d8f9c718bfd649ff1670a9341a"><td class="memTemplParams" colspan="2">template&lt;class Function &gt; </td></tr>
<tr class="memitem:a3a3287d8f9c718bfd649ff1670a9341a"><td class="memTemplItemLeft" align="right" valign="top"><a class="el" href="classcommon_1_1ScopeGuard.html">ScopeGuard</a>&lt; Function &gt;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a3a3287d8f9c718bfd649ff1670a9341a">makeGuard</a> (Function f)</td></tr>
<tr class="separator:a3a3287d8f9c718bfd649ff1670a9341a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adcb87d5a1ed7b254de78cdd03768456c"><td class="memTemplParams" colspan="2">template&lt;class T , class Compare &gt; </td></tr>
<tr class="memitem:adcb87d5a1ed7b254de78cdd03768456c"><td class="memTemplItemLeft" align="right" valign="top">constexpr const T &amp;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#adcb87d5a1ed7b254de78cdd03768456c">clamp</a> (const T &amp;v, const T &amp;lo, const T &amp;hi, Compare comp)</td></tr>
<tr class="separator:adcb87d5a1ed7b254de78cdd03768456c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a128a5ba755b87dccd7c0b847ad8a1187"><td class="memTemplParams" colspan="2">template&lt;class T &gt; </td></tr>
<tr class="memitem:a128a5ba755b87dccd7c0b847ad8a1187"><td class="memTemplItemLeft" align="right" valign="top">constexpr const T &amp;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a128a5ba755b87dccd7c0b847ad8a1187">clamp</a> (const T &amp;v, const T &amp;lo, const T &amp;hi)</td></tr>
<tr class="separator:a128a5ba755b87dccd7c0b847ad8a1187"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adf2a5c9681cae3e16719e17acf02f700"><td class="memTemplParams" colspan="2">template&lt;class T &gt; </td></tr>
<tr class="memitem:adf2a5c9681cae3e16719e17acf02f700"><td class="memTemplItemLeft" align="right" valign="top">constexpr const T &amp;&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#adf2a5c9681cae3e16719e17acf02f700">safeClamp</a> (const T &amp;v1, const T &amp;v2, const T &amp;v3)</td></tr>
<tr class="separator:adf2a5c9681cae3e16719e17acf02f700"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2c46570ad49cbc551055bff4659d8732"><td class="memItemLeft" align="right" valign="top">std::string&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a2c46570ad49cbc551055bff4659d8732">randId</a> (size_t len)</td></tr>
<tr class="separator:a2c46570ad49cbc551055bff4659d8732"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa3e430e5acdf3ddccb8364632331a226"><td class="memTemplParams" colspan="2">template&lt;typename Iter , typename RandomGenerator &gt; </td></tr>
<tr class="memitem:aa3e430e5acdf3ddccb8364632331a226"><td class="memTemplItemLeft" align="right" valign="top">Iter&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#aa3e430e5acdf3ddccb8364632331a226">select_randomly</a> (Iter start, Iter end, RandomGenerator &amp;g)</td></tr>
<tr class="separator:aa3e430e5acdf3ddccb8364632331a226"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7abe29f8c8599913843638b92bd7da90"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a7abe29f8c8599913843638b92bd7da90">stringSplit</a> (char const *str, size_t len, char sep, size_t max)</td></tr>
<tr class="memdesc:a7abe29f8c8599913843638b92bd7da90"><td class="mdescLeft">&#160;</td><td class="mdescRight">Split a string into parts deliminted by the given separtion character.  <a href="#a7abe29f8c8599913843638b92bd7da90">More...</a><br /></td></tr>
<tr class="separator:a7abe29f8c8599913843638b92bd7da90"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae13b32c492cf8afbbfb03c3b88ba6f4e"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ae13b32c492cf8afbbfb03c3b88ba6f4e">stringSplit</a> (char const *str, char sep, size_t max)</td></tr>
<tr class="separator:ae13b32c492cf8afbbfb03c3b88ba6f4e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad24b3866df5cee2212d9371efe4d04c9"><td class="memItemLeft" align="right" valign="top">std::vector&lt; std::string &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#ad24b3866df5cee2212d9371efe4d04c9">stringSplit</a> (std::string const &amp;str, char sep, size_t max)</td></tr>
<tr class="separator:ad24b3866df5cee2212d9371efe4d04c9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a399f3add34180341821fa6f7209d4942"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a399f3add34180341821fa6f7209d4942">startsWith</a> (std::string const &amp;str, std::string const &amp;prefix)</td></tr>
<tr class="separator:a399f3add34180341821fa6f7209d4942"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abead708a014f4c347dec6ac48b93ad5f"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#abead708a014f4c347dec6ac48b93ad5f">endsWith</a> (std::string const &amp;str, std::string const &amp;suffix)</td></tr>
<tr class="separator:abead708a014f4c347dec6ac48b93ad5f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a767e454a2f252441a4be39947bc152cf"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a767e454a2f252441a4be39947bc152cf">gmatch</a> (std::string_view str, std::string_view pattern)</td></tr>
<tr class="memdesc:a767e454a2f252441a4be39947bc152cf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Glob-style pattern matching.  <a href="#a767e454a2f252441a4be39947bc152cf">More...</a><br /></td></tr>
<tr class="separator:a767e454a2f252441a4be39947bc152cf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1c6ec3d2495fdcc02fcf0497c2872ed7"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a1c6ec3d2495fdcc02fcf0497c2872ed7">gmatchi</a> (std::string_view str, std::string_view pattern)</td></tr>
<tr class="memdesc:a1c6ec3d2495fdcc02fcf0497c2872ed7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Glob-style pattern matching (case-insensitive)  <a href="#a1c6ec3d2495fdcc02fcf0497c2872ed7">More...</a><br /></td></tr>
<tr class="separator:a1c6ec3d2495fdcc02fcf0497c2872ed7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6e53c5462aaa3a255598328c74f7a6ad"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a6e53c5462aaa3a255598328c74f7a6ad"><td class="memTemplItemLeft" align="right" valign="top">std::string&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a6e53c5462aaa3a255598328c74f7a6ad">stringToLower</a> (T &amp;&amp;str)</td></tr>
<tr class="separator:a6e53c5462aaa3a255598328c74f7a6ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a47ed567becb9c0777c4a5ce7b852e151"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a47ed567becb9c0777c4a5ce7b852e151"><td class="memTemplItemLeft" align="right" valign="top">std::string&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a47ed567becb9c0777c4a5ce7b852e151">joinVector</a> (std::vector&lt; T &gt; const &amp;v, char sep)</td></tr>
<tr class="separator:a47ed567becb9c0777c4a5ce7b852e151"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="var-members"></a>
Variables</h2></td></tr>
<tr class="memitem:a8909741e72f3f5df7ad5e922ac088930"><td class="memItemLeft" align="right" valign="top">auto const&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespacecommon.html#a8909741e72f3f5df7ad5e922ac088930">DataReader_NoopF</a> = [] {}</td></tr>
<tr class="separator:a8909741e72f3f5df7ad5e922ac088930"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>General utilities. </p>
</div><h2 class="groupheader">Typedef Documentation</h2>
<a class="anchor" id="a8798ccb7b38e02568bc3766dfc43fe2f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespacecommon.html#a8798ccb7b38e02568bc3766dfc43fe2f">common::DataReaderThreadInitF</a> = typedef std::function&lt;void()&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a647d70706422d1ce42ca7be76bf0ca37"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespacecommon.html#a647d70706422d1ce42ca7be76bf0ca37">common::hires_clock</a> = typedef std::chrono::steady_clock</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a3c23509b882f8742d58c9c13e002702e"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespacecommon.html#a3c23509b882f8742d58c9c13e002702e">common::HookFunction</a> = typedef std::function&lt;<a class="el" href="namespacecommon.html#adedd8ac6c51fc369a724bc5610779ffb">VarList</a>(const <a class="el" href="namespacecommon.html#adedd8ac6c51fc369a724bc5610779ffb">VarList</a>&amp;, const <a class="el" href="namespacecommon.html#adedd8ac6c51fc369a724bc5610779ffb">VarList</a>&amp;)&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a54cd622e23826275f4fc3b580c73dcf0"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespacecommon.html#a54cd622e23826275f4fc3b580c73dcf0">common::TensorTransform</a> = typedef std::function&lt;torch::Tensor(torch::Tensor)&gt;</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This is a convenience function to apply a tensor transformation to a complex type. </p>
<p>For example, you would like to write something like t = t.view(-1), but t is a tensor_list (and you'd like the operation to be applied to each element of the list). You can write instead t = applyTransform(t, [](torch::Tensor t){return t.view(-1);}); </p>

</div>
</div>
<a class="anchor" id="adedd8ac6c51fc369a724bc5610779ffb"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">using <a class="el" href="namespacecommon.html#adedd8ac6c51fc369a724bc5610779ffb">common::VarList</a> = typedef torch::autograd::variable_list</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<h2 class="groupheader">Enumeration Type Documentation</h2>
<a class="anchor" id="a95e9c4d7ebaf91bf709c582c45fa4ed6"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="namespacecommon.html#a95e9c4d7ebaf91bf709c582c45fa4ed6">common::ConcatType</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">strong</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a class="anchor" id="a95e9c4d7ebaf91bf709c582c45fa4ed6a6adf97f83acf6453d4a6a4b1070f3754"></a>None&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="a95e9c4d7ebaf91bf709c582c45fa4ed6a324118a6721dd6b8a9b9f4e327df2bf5"></a>Input&#160;</td><td class="fielddoc">
<p>No concatenation. </p>
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="a95e9c4d7ebaf91bf709c582c45fa4ed6a2403def5083f02105e7802b3b315681e"></a>Mirror&#160;</td><td class="fielddoc">
<p>Always concatenate input. </p>
<p>Concatenate input of mirror layer </p>
</td></tr>
</table>

</div>
</div>
<a class="anchor" id="a5ce7b02761744f38a15673e3fe0a4552"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="namespacecommon.html#a5ce7b02761744f38a15673e3fe0a4552">common::DecodeType</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">strong</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a class="anchor" id="a5ce7b02761744f38a15673e3fe0a4552a6adf97f83acf6453d4a6a4b1070f3754"></a>None&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="a5ce7b02761744f38a15673e3fe0a4552acc31a669f6d086f816852dda3290a3ef"></a>Conv&#160;</td><td class="fielddoc">
<p>No decoding. </p>
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="a5ce7b02761744f38a15673e3fe0a4552ad0f139c3cd10333e40bd51299c64d30a"></a>Deconv&#160;</td><td class="fielddoc">
<p>Decode with convolutions. </p>
<p>Decode with transposed convolutions </p>
</td></tr>
</table>

</div>
</div>
<a class="anchor" id="a6c60ad4086ad27183b4794f0844e73d1"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="namespacecommon.html#a6c60ad4086ad27183b4794f0844e73d1">common::DilationScheme</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">strong</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a class="anchor" id="a6c60ad4086ad27183b4794f0844e73d1a6adf97f83acf6453d4a6a4b1070f3754"></a>None&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="a6c60ad4086ad27183b4794f0844e73d1a32a843da6ea40ab3b17a3421ccdf671b"></a>Linear&#160;</td><td class="fielddoc">
<p>No dilation. </p>
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="a6c60ad4086ad27183b4794f0844e73d1ac1e19c09f700938f0ff7f1fd4722a3ac"></a>Exponential&#160;</td><td class="fielddoc">
<p>The dilation increases linearly at each layer. </p>
<p>The dilation increases exponentially </p>
</td></tr>
</table>

</div>
</div>
<a class="anchor" id="a5d2f6ab6c2580a0834320dc08c87e279"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="namespacecommon.html#a5d2f6ab6c2580a0834320dc08c87e279">common::PadType</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">strong</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a class="anchor" id="a5d2f6ab6c2580a0834320dc08c87e279ad7ed4ee1df437474d005188535f74875"></a>Zero&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="a5d2f6ab6c2580a0834320dc08c87e279aaea1e492943ccbad7ee270ec1e064758"></a>Reflection&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="a5d2f6ab6c2580a0834320dc08c87e279a8c340dc334134096f68b880b42a8692c"></a>Replication&#160;</td><td class="fielddoc">
</td></tr>
</table>

</div>
</div>
<a class="anchor" id="aee0ad6b31a5c2edcebea2c9745660725"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725">common::UpsampleMode</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">strong</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Mimics pytorch's upsample function. </p>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a class="anchor" id="aee0ad6b31a5c2edcebea2c9745660725a60494f02d440f316319dd0fad40ad007"></a>Nearest&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="aee0ad6b31a5c2edcebea2c9745660725a32a843da6ea40ab3b17a3421ccdf671b"></a>Linear&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="aee0ad6b31a5c2edcebea2c9745660725aaf17c98bbd83c27d6426d2ff3fa81d7f"></a>Bilinear&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="aee0ad6b31a5c2edcebea2c9745660725a969a7b6f1f3951ef293b3ff281dc293d"></a>Trilinear&#160;</td><td class="fielddoc">
</td></tr>
</table>

</div>
</div>
<a class="anchor" id="ae0762fe221f7fddd392f24cbae4d649c"></a>
<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="namespacecommon.html#ae0762fe221f7fddd392f24cbae4d649c">common::UpsamplingType</a></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">strong</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><a class="anchor" id="ae0762fe221f7fddd392f24cbae4d649ca6adf97f83acf6453d4a6a4b1070f3754"></a>None&#160;</td><td class="fielddoc">
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="ae0762fe221f7fddd392f24cbae4d649ca18c441822997cef7f104da468455fb88"></a>Bilin&#160;</td><td class="fielddoc">
<p>No upsampling. </p>
</td></tr>
<tr><td class="fieldname"><a class="anchor" id="ae0762fe221f7fddd392f24cbae4d649cad0f139c3cd10333e40bd51299c64d30a"></a>Deconv&#160;</td><td class="fielddoc">
<p>Bilinear upsampling (fixed) </p>
<p>Learnt upsampling (transposed convolution) </p>
</td></tr>
</table>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a class="anchor" id="afa92f86a81bea052e6adad0c0c39f281"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor const &amp; common::addHook </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor const &amp;&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecommon.html#a3c23509b882f8742d58c9c13e002702e">HookFunction</a> &amp;&amp;&#160;</td>
          <td class="paramname"><em>f</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a hook to the backwards of the variable. </p>
<p>The hook function takes gradInput and gradOutput, and should by default return gradInput, that is, the identity function looks like: [](VarList const&amp; gradInp, Varlist const&amp; gradOutp) { return gradInp; }</p>
<p><a href="https://pytorch.org/docs/stable/nn.html?highlight=hook#torch.nn.Module.register_backward_hook">https://pytorch.org/docs/stable/nn.html?highlight=hook#torch.nn.Module.register_backward_hook</a> </p>

</div>
</div>
<a class="anchor" id="a2ab3d9995a952d2e4c4c2bab436f0541"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">ag::Variant common::applyTransform </td>
          <td>(</td>
          <td class="paramtype">ag::Variant&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="namespacecommon.html#a54cd622e23826275f4fc3b580c73dcf0">TensorTransform</a> &amp;&#160;</td>
          <td class="paramname"><em>fun</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a1722115063aa872f70355ab48c2b0c4d"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void common::assertSize </td>
          <td>(</td>
          <td class="paramtype">const std::string &amp;&#160;</td>
          <td class="paramname"><em>name</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const torch::Tensor &amp;&#160;</td>
          <td class="paramname"><em>tensor</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::IntList&#160;</td>
          <td class="paramname"><em>sizes</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Verifies that a tensor's dimension sizes match expectations. </p>
<p>If a dimension is negative (e.g. -1) it won't be checked. Throws a std::range_error if they don't. </p>

</div>
</div>
<a class="anchor" id="a3d897b66ebb1c87646c385f31de4c99c"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">common::AUTOGRAD_CONTAINER_CLASS </td>
          <td>(</td>
          <td class="paramtype">MLP&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Simple MLP of nLayers layers, with hidden size all being the same. </p>
<p>Optionally, we can zero the last layer, which is useful if the output is suppose to be a probability distribution since values will be uniform after softmax </p>

</div>
</div>
<a class="anchor" id="a18d21af0bb1a1bd51acf6589eaf8fc05"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">common::AUTOGRAD_CONTAINER_CLASS </td>
          <td>(</td>
          <td class="paramtype">GatedConv&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a6d25c1fff03a1b36a30cd19be24b2241"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">common::AUTOGRAD_CONTAINER_CLASS </td>
          <td>(</td>
          <td class="paramtype">ConvBlock&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Simple convolutional block, with optional residual connection From a user perspective, the convolution parameters behave the same as if the block was a single conv layer. </p>
<p>For example if the stride is 2, the output will be twice smaller than the input, irrespective of the number of inner layers. In practice the stride and dilation are only applied to the first layer. The block also applies padding to compensate for the kernel size and the dilation. That means that if the input is size hxw, the output will be h'xw' with h' = (h - 1)/stride + 1 and w' = (w-1)/stride + 1 </p>
<p>Number of feature channels in the input</p>
<p>Number of feature channels in the output</p>
<p>Non linearity inserted between each convolution</p>
<p>If true, the module performs transposed convolutions instead</p>
<p>Size of the convolution kernels (we use kernelSize X kernelSize)</p>
<p>Stride of the convolutions</p>
<p>Dilation of the convolutions</p>
<p>Add a residual convolution when true</p>
<p>Add a batchNorm layers where appropriate, if true</p>
<p>If true, the intermediate convolutions will have 4 times less features than the output</p>
<p>Number of convolution layers</p>
<p>Bias in the convolutions</p>
<p>Whether to use gated convolutions</p>
<p>How to pad </p>

</div>
</div>
<a class="anchor" id="aedab797eb4cdaf2aaf86b69f3ec3f44b"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">common::AUTOGRAD_CONTAINER_CLASS </td>
          <td>(</td>
          <td class="paramtype">EncoderDecoder&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Shape of the input, given as [c,h,w], where c is the number of channels, h is the height and w the width</p>
<p>Number of feature channels in the intermediate layers</p>
<p>Number of feature channels in the output</p>
<p>Non linearity inserted between each convolution</p>
<p>Strategy for concatening previous layers during decoding</p>
<p>Strategy for upsampling, when needed</p>
<p>Strategy for decoding</p>
<p>Strategy for dilation</p>
<p>Size of the convolution kernels (we use kernelSize X kernelSize)</p>
<p>Stride of the convolutions</p>
<p>Add a residual convolution when true</p>
<p>Add a batchNorm layers where appropriate, if true</p>
<p>If true, the intermediate convolutions will have 4 times less features than the output</p>
<p>Number of Convolutional blocks in the encoding (if there is decoding, it will contain the same amount of blocks)</p>
<p>Number of convolution layers in each block</p>
<p>Bias in the convolutions</p>
<p>Whether to use gated convolutions </p>

</div>
</div>
<a class="anchor" id="a1f46d58d0ede06bfef969550066be329"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">common::AUTOGRAD_CONTAINER_CLASS </td>
          <td>(</td>
          <td class="paramtype">MHAttention&#160;</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="aff3cdc4a58d5aaab088d23646d57da47"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void common::checkTensor </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>logOnError</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Throws if the given float tensor has a NaN or +/- infinity. </p>

</div>
</div>
<a class="anchor" id="adcb87d5a1ed7b254de78cdd03768456c"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T , class Compare &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">constexpr const T&amp; common::clamp </td>
          <td>(</td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>v</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>lo</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>hi</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Compare&#160;</td>
          <td class="paramname"><em>comp</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a128a5ba755b87dccd7c0b847ad8a1187"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">constexpr const T&amp; common::clamp </td>
          <td>(</td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>v</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>lo</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>hi</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a0cedd9ef57020257a649a35cda020ec6"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void common::clipGradientNorms </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; torch::Tensor &gt;&#160;</td>
          <td class="paramname"><em>parameters</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>maxNorm</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Rescale gradients so that the norm of all gradients (concatenated) is smaller than maxNorm. </p>

</div>
</div>
<a class="anchor" id="a328822315f7b76ea5fd7afe902a9d174"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">backward::StackTrace common::createStackTrace </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a521443beb7c0ef5abf09b5b27bf53b98"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::crossEntropyLoss </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dim</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Reduction::Reduction&#160;</td>
          <td class="paramname"><em>reduction</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="abead708a014f4c347dec6ac48b93ad5f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool common::endsWith </td>
          <td>(</td>
          <td class="paramtype">std::string const &amp;&#160;</td>
          <td class="paramname"><em>str</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::string const &amp;&#160;</td>
          <td class="paramname"><em>suffix</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a34d4ac17f6acdebb57a44e605866dda9"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Enumeration &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">auto common::enumAsInt </td>
          <td>(</td>
          <td class="paramtype">Enumeration const&#160;</td>
          <td class="paramname"><em>value</em></td><td>)</td>
          <td> -&gt; typename std::underlying_type&lt;Enumeration&gt;::type </td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a2028ee778450a8f8c61a82d3990acea7"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::extendIndex </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>axis</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>d</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns a byte tensor x such that selectIndex(x, y, axis) are only 1s. </p>
<p>y is of shape ... x 1 x ... x is of shape ... x d x ... </p>

</div>
</div>
<a class="anchor" id="aaa55ed98e60017f41cddd7936c7468e2"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::flip </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dim</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Flips a tensor along a given dimension. </p>
<p>y[-a-,i,-b-] = x[-a-,n-i-1,-b-] </p>

</div>
</div>
<a class="anchor" id="a048a13249fa22958d877f55a96f8c585"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Device common::getVariantDevice </td>
          <td>(</td>
          <td class="paramtype">ag::Variant&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Utility to get the device of a variant. </p>
<p>If the variants contains several tensors, we assume they have the same device </p>

</div>
</div>
<a class="anchor" id="a767e454a2f252441a4be39947bc152cf"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool common::gmatch </td>
          <td>(</td>
          <td class="paramtype">std::string_view&#160;</td>
          <td class="paramname"><em>str</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::string_view&#160;</td>
          <td class="paramname"><em>pattern</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Glob-style pattern matching. </p>

</div>
</div>
<a class="anchor" id="a1c6ec3d2495fdcc02fcf0497c2872ed7"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool common::gmatchi </td>
          <td>(</td>
          <td class="paramtype">std::string_view&#160;</td>
          <td class="paramname"><em>str</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::string_view&#160;</td>
          <td class="paramname"><em>pattern</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Glob-style pattern matching (case-insensitive) </p>

</div>
</div>
<a class="anchor" id="a4bdcc11eaa0149740bea8a2503662758"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool common::gpuAvailable </td>
          <td>(</td>
          <td class="paramname"></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Checks if a CUDA GPU is available. </p>

</div>
</div>
<a class="anchor" id="a06a7d0810e5cfcf017cb55534743f2ad"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::indexMean </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dim</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>source</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Like zeros.index_add_ but with the mean. </p>
<p>source has shape X1 x ... Xdim-1 x N x Xdim+1 x ... Xd index has shape N, with values ranging from 0 to size - 1 x (return value) has shape X1 x ... Xdim-1 x size x Xdim+1 x ... x Xd x[-a-,i,-b-] is the mean of {source[-a-,j,-b-] where index[j]=i} and zero if this set is empty. </p>

</div>
</div>
<a class="anchor" id="a47ed567becb9c0777c4a5ce7b852e151"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">std::string common::joinVector </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; T &gt; const &amp;&#160;</td>
          <td class="paramname"><em>v</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">char&#160;</td>
          <td class="paramname"><em>sep</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a927751b4aecba3af139b560226d25d32"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::makeBatch </td>
          <td>(</td>
          <td class="paramtype">ag::tensor_list const &amp;&#160;</td>
          <td class="paramname">, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>pad</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Equivalent to a stack along dim 0 of the input, but with the values padded correct so the size is rectangular. </p>
<p>For example, if the list has size [(6, 2), (5, 2), (7, 3)] The result is a tensor of (3, 7, 3) </p>

</div>
</div>
<a class="anchor" id="a0f02d76dc0416dbe2e684e40eda6fc92"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">ag::Variant common::makeBatchVariant </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; ag::Variant &gt; &amp;&#160;</td>
          <td class="paramname"><em>queries</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>pad</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function works similarly as makeBatch but handles more input types. </p>
<p>The queries variants are requested to be the same type (we tolerate to mix tensors and size 1 tensor_list) It behaves as follows, depending on the variant type:</p><ul>
<li>If the queries are tensors, it calls makeBatch and returns a tensor</li>
<li>If the queries are dict, it batches each key individually, and returns a dict. Note that all queries must contain the same keys</li>
<li>If the queriest are a tensor_list, it batches each item of the list individually, and returns a tensor_list. Note that all the queries must have the same number of tensors, semantically in the same order. </li>
</ul>

</div>
</div>
<a class="anchor" id="a0812ae93030aec1c6d9d4d873fe29271"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">auto common::makeDataReader </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; std::string &gt;&#160;</td>
          <td class="paramname"><em>paths</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>numThreads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>batchSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::string&#160;</td>
          <td class="paramname"><em>pathPrefix</em> = <code>std::string()</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecommon.html#a8798ccb7b38e02568bc3766dfc43fe2f">DataReaderThreadInitF</a>&#160;</td>
          <td class="paramname"><em>init</em> = <code><a class="el" href="namespacecommon.html#a8909741e72f3f5df7ad5e922ac088930">DataReader_NoopF</a></code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="ac749eb34bb70eb7a2e383f499c781f48"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T , typename F &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">auto common::makeDataReader </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; std::string &gt;&#160;</td>
          <td class="paramname"><em>paths</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>numThreads</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>batchSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">F&#160;</td>
          <td class="paramname"><em>transform</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::string&#160;</td>
          <td class="paramname"><em>pathPrefix</em> = <code>std::string()</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecommon.html#a8798ccb7b38e02568bc3766dfc43fe2f">DataReaderThreadInitF</a>&#160;</td>
          <td class="paramname"><em>init</em> = <code><a class="el" href="namespacecommon.html#a8909741e72f3f5df7ad5e922ac088930">DataReader_NoopF</a></code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="ac1a19e3dbacb1000f49d696cec022e30"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T , typename F &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">std::unique_ptr&lt;<a class="el" href="classcommon_1_1DataReaderTransform.html">DataReaderTransform</a>&lt;T, F&gt; &gt; common::makeDataReaderTransform </td>
          <td>(</td>
          <td class="paramtype">std::unique_ptr&lt; <a class="el" href="classcommon_1_1DataReaderIterator.html">DataReaderIterator</a>&lt; T &gt;&gt; &amp;&amp;&#160;</td>
          <td class="paramname"><em>it</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">F &amp;&amp;&#160;</td>
          <td class="paramname"><em>function</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecommon.html#a8798ccb7b38e02568bc3766dfc43fe2f">DataReaderThreadInitF</a>&#160;</td>
          <td class="paramname"><em>init</em> = <code><a class="el" href="namespacecommon.html#a8909741e72f3f5df7ad5e922ac088930">DataReader_NoopF</a></code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a3a3287d8f9c718bfd649ff1670a9341a"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class Function &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classcommon_1_1ScopeGuard.html">ScopeGuard</a>&lt;Function&gt; common::makeGuard </td>
          <td>(</td>
          <td class="paramtype">Function&#160;</td>
          <td class="paramname"><em>f</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="acaa59964ea24462c5299cd13d50fac9e"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::maskedCopy </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>source</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Immutable masked copy (equivalent to x.clone().<a class="el" href="namespacecommon.html#ab40691e6a2589e90b0217ab16bf71a1d" title="For 1D tensors, this is equivalent to: x[i] &lt;- source[i] if mask[i] == 1. ">maskedCopy_()</a>). </p>
<p>NOTE: this does not work if x contains NaNs or infinities! mask should have the same type as x. </p>

</div>
</div>
<a class="anchor" id="ab40691e6a2589e90b0217ab16bf71a1d"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void common::maskedCopy_ </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>source</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>For 1D tensors, this is equivalent to: x[i] &lt;- source[i] if mask[i] == 1. </p>

</div>
</div>
<a class="anchor" id="a0f52ff6fc24f7464958070dd246611b4"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; torch::Tensor, torch::Tensor &gt; common::maskedMax </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dim</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>keepDim</em> = <code>false</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute a masked max/argmax of a tensor. </p>
<p>The passed in mask must be a variable of 0.0's and 1.0's (floats) of the same shape as the input.</p>
<ul>
<li>input is a float tensor of the same shape as the mask</li>
<li>mask is a binary float tensor of the same shape as the input</li>
<li>dim is the dimension along which to apply the softmax</li>
<li>keepDim is whether to squeeze the resulting tensors or not</li>
</ul>
<p>Returns the output after masking and softmaxing. NOTE: behavior is undefined if mask is zero for some batch. </p>

</div>
</div>
<a class="anchor" id="a596edcc9ffdf5ddd17f2872c9dee4771"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::maskedMean </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Average x over non-masked indices, returning 0 if all indices are masked. </p>
<p>This does not work if x contains NaNs or infinities at masked indices.</p>
<ul>
<li>mask should be expandable to x's shape</li>
<li>returns a scalar which is a masked average of x. </li>
</ul>

</div>
</div>
<a class="anchor" id="af50d71f082fad6c77806afb11e7977ed"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::maskedSoftmax </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dim</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>clampEpsilon</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute a masked softmax of a tensor in a numerically stable way by removing the max value before exponentiating. </p>
<p>The passed in mask must be a variable of 0.0's and 1.0's (floats) of the same shape as the input.</p>
<ul>
<li>input is a float tensor of the same shape as the mask</li>
<li>mask is a binary float tensor of the same shape as the input</li>
<li>dim is the dimension along which to apply the softmax</li>
<li>clampEpsilon is the minimum value to clamp the output to</li>
</ul>
<p>Returns the output after masking and softmaxing. </p>

</div>
</div>
<a class="anchor" id="a3c7fced80a0a71cdfd8a464ab71243a8"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::maskedSum </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Sum x across non-masked indices. </p>
<p>This does not work if x contains NaNs or infinities at masked indices.</p>
<ul>
<li>mask should be expandable to x's shape</li>
<li>returns a scalar which is a masked sum of x. </li>
</ul>

</div>
</div>
<a class="anchor" id="a0715f0339bc9259e43162035e313c204"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint8_t&gt; common::md5sum </td>
          <td>(</td>
          <td class="paramtype">std::string_view&#160;</td>
          <td class="paramname"><em>data</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a4963b461ac47972bfa158fb47d51a281"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint8_t&gt; common::md5sum </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; uint8_t &gt; const &amp;&#160;</td>
          <td class="paramname"><em>data</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a943de486e4b8b0fa227d44caa64702cd"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; uint8_t &gt; common::md5sum </td>
          <td>(</td>
          <td class="paramtype">void const *&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>len</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="ae17e8aba9ee9fcc37164ac86fce0b52b"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::meshGrid </td>
          <td>(</td>
          <td class="paramtype">ag::tensor_list&#160;</td>
          <td class="paramname"><em>tensors</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Takes N 1D tensors xi of size Xi and returns a tensor y of size X1 x ... </p>
<p>x XN x N such that y[a1]...[aN][i] = xi[ai]. </p>

</div>
</div>
<a class="anchor" id="aa3a5ffdd16f2fbc577c2e7585826437d"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::mseLoss </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>sizeAverage</em> = <code>true</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>reduce</em> = <code>true</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Computes the MSE loss between x and y. </p>
<ul>
<li>x and y must have same shape, and mask be expandable to x's shape</li>
<li>if reduce is true, losses will be summed or averaged depending on sizeAverage (averaged over non-masked indices)</li>
<li>returns a scalar if reduce is true, otherwise a x-like tensor (with zeros at masked indices) </li>
</ul>

</div>
</div>
<a class="anchor" id="af0f332c7ece355863f008a6926bca6b1"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::nllLoss </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dim</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>target</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>weight</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Reduction::Reduction&#160;</td>
          <td class="paramname"><em>reduction</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="aafe019d691deac5c07646f046796bc9f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::normalPDF </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mean</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>std</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the PDF of the normal law. </p>

</div>
</div>
<a class="anchor" id="af47e9d6eb984101dd1ca8602260acf7b"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::normalPDF </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mean</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>std</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="ab3207d3b5ed49cd34d43d779a1c5bfda"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::ostream &amp; common::operator&lt;&lt; </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;&#160;</td>
          <td class="paramname"><em>out</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="structcommon_1_1WeightSummary.html">WeightSummary</a> &amp;&#160;</td>
          <td class="paramname"><em>summary</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a0206c43d70412a65070cc5fe14c31104"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::pad2d </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::IntList&#160;</td>
          <td class="paramname"><em>pad</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Zero-padding (only supports 3d input) </p>

</div>
</div>
<a class="anchor" id="a8491908e5c44ef8d3b1dcb26de9a4c68"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::padNd </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::IntList&#160;</td>
          <td class="paramname"><em>pad</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Zero-padding (for any number of dimensions) For every dimensions of the input, pad contains 2 elements: the padding before and after along this dimension. </p>

</div>
</div>
<a class="anchor" id="a3d29882328849ffa17e1223e28d3a154"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void common::putNd_ </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>index</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>source</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>accumulate</em> = <code>false</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Copies elements from source into x at positions determined by index. </p>
<p>If accumulate is true, adds instead of copy (otherwise, indices should appear at most once in index). x has shape X1 x .. x XD index has shape N x D source has shape N For 2D tensors, this is equivalent to: x[index[i][0], index[i][1]] &lt;- source[i] </p>

</div>
</div>
<a class="anchor" id="a2c46570ad49cbc551055bff4659d8732"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string common::randId </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>len</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="aa6c41f5406c455f82089b64539ccde55"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::repeat2d </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::IntList&#160;</td>
          <td class="paramname"><em>sizes</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Repeat a 1D tensor so that you end up with a (#channels, sizes[0], size[1]) tensor. </p>

</div>
</div>
<a class="anchor" id="adf2a5c9681cae3e16719e17acf02f700"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">constexpr const T&amp; common::safeClamp </td>
          <td>(</td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>v1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>v2</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>v3</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a1d8c2672f626cd80d6dfb68c26fef6b7"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::scatterSum2d </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>positions</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::IntList&#160;</td>
          <td class="paramname"><em>sizes</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Scatter data into dest at given positions. </p>
<p>Depending on device that data lives on, different algorithms will be used:</p><ul>
<li>For CPU tensors, multiple scatter_add_ calls are used in order to sum up channels for duplicate positions</li>
<li>For GPU tensors, data will be scattered onto all planes in a single scatter_ call and summed up afterwards. This means the function will allocate an intermediate buffer of size (b, c, np, H, W) if there are no more than np duplicate positions.</li>
</ul>
<p>There's a benchmark for this function in the corresponding unit tests.</p>
<p>positions is a (b, n, 2) integer tensor with elements greater than or equal to zero. positions[i][0] refers to the Y position, positions[i][1] to the X position of the data entry i. data is a (b, n, c) tensor. Each of the n entries will be placed in dest according to the respective position. Entries on each batch until the first negative entry will be considered. sizes is the {H, W} tuple of the size of the plane to scatter onto.</p>
<p>For single element, it's sufficient to unsqueeze(0) for it to look batched. Positions don't have to be unique &ndash; this function performs sum-pooling by default. The output is of size (b, c, y, x), similar to the input to a convnet. </p>

</div>
</div>
<a class="anchor" id="aa3e430e5acdf3ddccb8364632331a226"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Iter , typename RandomGenerator &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">Iter common::select_randomly </td>
          <td>(</td>
          <td class="paramtype">Iter&#160;</td>
          <td class="paramname"><em>start</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Iter&#160;</td>
          <td class="paramname"><em>end</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">RandomGenerator &amp;&#160;</td>
          <td class="paramname"><em>g</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="ad93f6d196233fabf025480c20f3d6b2a"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::selectIndex </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>axis</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>keepDim</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a1de170b4b067d43d139ae986bf78c842"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint8_t&gt; common::sha256sum </td>
          <td>(</td>
          <td class="paramtype">std::string_view&#160;</td>
          <td class="paramname"><em>data</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a5c52510f1a58f2330aa65d0800f78395"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;uint8_t&gt; common::sha256sum </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; uint8_t &gt; const &amp;&#160;</td>
          <td class="paramname"><em>data</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a38c1a0bfca2038eb61540f16b4f3bb39"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; uint8_t &gt; common::sha256sum </td>
          <td>(</td>
          <td class="paramtype">void const *&#160;</td>
          <td class="paramname"><em>data</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>len</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a0eb9d40d761b11d3f9d6dd99774af699"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::squash </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>j</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Squash contiguous dimensions of a tensor into a single dimension. </p>
<p>The dimensions [i..j] (both included) will be squashed into a single one. So if x is of size s_1 x ... x s_d, the returned tensor will be a view of x of size s_1 x ... x s_i-1 x s_i * s_i+1 * ... * s_j x s_j+1 x ... x s_d. </p>

</div>
</div>
<a class="anchor" id="a399f3add34180341821fa6f7209d4942"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">bool common::startsWith </td>
          <td>(</td>
          <td class="paramtype">std::string const &amp;&#160;</td>
          <td class="paramname"><em>str</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::string const &amp;&#160;</td>
          <td class="paramname"><em>prefix</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a7abe29f8c8599913843638b92bd7da90"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; std::string &gt; common::stringSplit </td>
          <td>(</td>
          <td class="paramtype">char const *&#160;</td>
          <td class="paramname"><em>str</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>len</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">char&#160;</td>
          <td class="paramname"><em>sep</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>max</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Split a string into parts deliminted by the given separtion character. </p>
<p>This will repeatedly call <code>getline()</code> with <code>sep</code> as the delimitation character. If <code>max</code> is &gt;= 0, at most <code>max</code> splits will be performed (cf. Python's split() function). </p>

</div>
</div>
<a class="anchor" id="ae13b32c492cf8afbbfb03c3b88ba6f4e"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; std::string &gt; common::stringSplit </td>
          <td>(</td>
          <td class="paramtype">char const *&#160;</td>
          <td class="paramname"><em>str</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">char&#160;</td>
          <td class="paramname"><em>sep</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>max</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="ad24b3866df5cee2212d9371efe4d04c9"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; std::string &gt; common::stringSplit </td>
          <td>(</td>
          <td class="paramtype">std::string const &amp;&#160;</td>
          <td class="paramname"><em>str</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">char&#160;</td>
          <td class="paramname"><em>sep</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"><em>max</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a6e53c5462aaa3a255598328c74f7a6ad"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">std::string common::stringToLower </td>
          <td>(</td>
          <td class="paramtype">T &amp;&amp;&#160;</td>
          <td class="paramname"><em>str</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="ae28cd3961778cddfa278b9ec8a9a7d8f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::takeNd </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>index</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Inverse operation of putNd_. </p>
<p>x has shape X1 x .. x Xd index has shape N x d y (return value) has shape N For 2D tensors, this is equivalent to: y[i] = x[index[i][0], index[i][1]]; </p>

</div>
</div>
<a class="anchor" id="abf55d6112d2ad4bcde92acb7098fdabe"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::tensorFromNpyArray </td>
          <td>(</td>
          <td class="paramtype">cnpy::NpyArray&#160;</td>
          <td class="paramname"><em>array</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::TensorOptions&#160;</td>
          <td class="paramname"><em>op</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a6403cdbd861177e3001b494300609130"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string common::tensorInfo </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns a string containing the tensor type and sizes. </p>

</div>
</div>
<a class="anchor" id="a5c00b2018ce9c66e0aea9248c7864378"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string common::tensorStats </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns a string containing the tensor info, the max/min/mean and sum. </p>

</div>
</div>
<a class="anchor" id="a63e26830a3ec74de71978e51f2163de3"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string common::toHex </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; uint8_t &gt; const &amp;&#160;</td>
          <td class="paramname"><em>digest</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="af6875c702673bffa084bc33a2b88d997"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; ag::Variant &gt; common::unBatchVariant </td>
          <td>(</td>
          <td class="paramtype">ag::Variant&#160;</td>
          <td class="paramname"><em>batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>stride</em> = <code>1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>maskOut</em> = <code>false</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>maskValue</em> = <code>-1</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>This function is the opposite of makeBatchVariant. </p>
<p>It assumes that the tensors to be found in the batch have a first dimension of size b, interpreted as the batch dimension. It will take slices of size </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname"></td><td></td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a3f4a250cfa835be53f41c68434c8d623"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::unsquash </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::IntList&#160;</td>
          <td class="paramname"><em>sizes</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Unsquash a dimension of a tensor into several dimensions. </p>
<p>Replace the i-th dimension of x by sizes (this augments the number of dimensions of x). The product of the elements of sizes should be x.size(i) (sizes can also contain a -1). If x is of size s_1 x ... x s_d, the returned tensor will be a view of x of size s_1 x ... x s_i-1 x sizes x s_i+1 x ... s_d. </p>

</div>
</div>
<a class="anchor" id="a1a22830e6cc488dfba4e61bdde64d127"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::unsqueezes </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>before</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>after</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Do multiple unsqueezes on first and last dimensions. </p>

</div>
</div>
<a class="anchor" id="af8711a9cfdf20b614f9d2236903252b0"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::upsample </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725">UpsampleMode</a>&#160;</td>
          <td class="paramname"><em>mode</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">at::IntList&#160;</td>
          <td class="paramname"><em>size</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="ae7d6f79d3075016c8ebfd2bb43857d93"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::upsample </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="namespacecommon.html#aee0ad6b31a5c2edcebea2c9745660725">UpsampleMode</a>&#160;</td>
          <td class="paramname"><em>mode</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>scaleFactor</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
<a class="anchor" id="a182601d099eaa2a84e1bca8cc8257bd8"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::string common::variantInfo </td>
          <td>(</td>
          <td class="paramtype">ag::Variant&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Returns a string describing the content of a variant. </p>

</div>
</div>
<a class="anchor" id="ae4caa699050fa90de074d7e0a506754a"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">torch::Tensor common::weightedMaskedSoftmax </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>input</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>mask</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"><em>dim</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float&#160;</td>
          <td class="paramname"><em>clampEpsilon</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute a weighted masked softmax of a tensor in a numerically stable way by removing the max value before exponentiating. </p>
<p>The passed in mask must be a variable of floats of the same shape as the input. It should include weighting and masking as desired (it need not be binary).</p>
<ul>
<li>input is a float tensor of the same shape as the mask</li>
<li>mask is a float tensor of the same shape as the input</li>
<li>dim is the dimension along which to apply the softmax</li>
<li>clampEpsilon is the minimum value to clamp the output to</li>
</ul>
<p>Returns the output after weighting, masking, and softmaxing. </p>

</div>
</div>
<a class="anchor" id="a4d08e8aa3221955ce42cc61e6424c26c"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void common::zerosToOnes_ </td>
          <td>(</td>
          <td class="paramtype">torch::Tensor&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Replace (in-place) all zeroes of x by ones. </p>

</div>
</div>
<h2 class="groupheader">Variable Documentation</h2>
<a class="anchor" id="a8909741e72f3f5df7ad5e922ac088930"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">auto const common::DataReader_NoopF = [] {}</td>
        </tr>
      </table>
</div><div class="memdoc">

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Wed Feb 20 2019 18:40:11 for TorchCraftAI by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.11
</small></address>
</body>
</html>
